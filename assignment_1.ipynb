{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data provenance and characteristics:\n",
    " \n",
    "### When and from where it was collected:\n",
    "Data was collected from the GitHub presented in moodle:  https://github.com/Jl-wei/APIA2022-French-user-reviews-classification-dataset\n",
    "\n",
    "### Text genre(s) and language(s) it covers\n",
    "The data is purely made up of reviews on the app store, all of it written in French.\n",
    "\n",
    "### How it has been annotated\n",
    "Each entry in the data set is annotated with four labels: \"Rating\", \"User Experience\", \"Feature request\" and \"Bug Report\"\n",
    "It also includes the score given by the user in each individual review.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data from the file\n",
    "data_garmin_df = pd.read_csv('data/Garmin_Connect.csv')\n",
    "data_samsung_df = pd.read_csv('data/Samsung_Health.csv')\n",
    "data_huawei_df = pd.read_csv('data/Huawei_Health.csv')\n",
    "\n",
    "data = pd.concat([data_garmin_df, data_samsung_df, data_huawei_df], ignore_index=True)\n",
    "#data.to_csv('data/concatenated_data.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(data.head(10))\n",
    "print(\"\\n Number of rows: \" + str(len(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Relevant plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "data_labels = data[[\"rating\", \"bug_report\", \"feature_request\", \"user_experience\"]]\n",
    "\n",
    "data_labels.sum(axis=0).plot.bar()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see, the rating label is much more relevant than the other 3 labels, which causes unbalance and might mislead the results if not careful. Also, it seems that there are cases where more than 1 label can be applied. This makes it a multilabel problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Types of classification problems (https://towardsdatascience.com/multilabel-text-classification-done-right-using-scikit-learn-and-stacked-generalization-f5df2defc3b5#6de1)](./data/Types_of_classification_problems.png \"Types of classification problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To try to make this problem a binary one, one must make it so that the classification only hits one of the many label columns. That can be achieved by making all possible combinations columns in their own right. Or we can make it into a multiclass one by joining every label as a string in a new column \"tags\" and try to predict it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#data.insert(2, \"Tags\", [' ' for _ in range(len(data.index))], True)\n",
    "\n",
    "tags = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    str_tag = \"\"\n",
    "    if row[\"rating\"] == 1:\n",
    "        str_tag += \"rating,\"\n",
    "    if row[\"bug_report\"] == 1:\n",
    "        str_tag += \"bug_report,\"\n",
    "    if row[\"feature_request\"] == 1:\n",
    "        str_tag += \"feature_request,\"\n",
    "    if row[\"user_experience\"] == 1:\n",
    "        str_tag += \"user_experience,\"\n",
    "\n",
    "    list_tag = str_tag[0:-1].split(',')\n",
    "\n",
    "    tags.append(list_tag)\n",
    "\n",
    "#print(tags)Â´\n",
    "\n",
    "data = data.assign(tags=tags)\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "tags1 = mlb.fit_transform(data[\"tags\"])\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "data[\"tags\"].value_counts().sort_index().plot.bar(x=\"Tag Distribution of All Observations\", y=\"Number of observations\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and removal of stopwords\n",
    "\n",
    "*Tokenization* is the process of splitting an input text into tokens (words or other relevant elements, such as punctuation, empty strings). We will use the result as a basis to predict a label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize #principal tokenization class from nltk API\n",
    "from nltk.stem import SnowballStemmer #Stemming method\n",
    "import re #regex library\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    review = re.sub('\\*', '', row[\"data\"]) # get data, substitute asterisks for empty string, put into review\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review) # from review, remove all non-alphabetic characters\n",
    "    review = re.sub('[^\\w\\s]', '', review) # remove punctuation from review\n",
    "    review = ' '.join([SnowballStemmer('french').stem(w) for w in word_tokenize(review.lower(), language='french')]) # apply stemming\n",
    "    corpus.append(review)\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "data = data.assign(token=corpus)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Separation between train and test datasets\n",
    "Separate in adequate proportions to avoid the overfitting of the modules the data between features and targets. In this case there will be 2 different separations, one for the original multilabel problem and another for the mold into just a multiclass problem. To ensure a more even tag distribution, we must use the *stratify* hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#features = ['token', 'score']\n",
    "\n",
    "#X = data.loc[:,features] #select from data the features\n",
    "\n",
    "X_train_mc, X_mc, y_train_mc, y_mc = train_test_split(\n",
    "    data[\"token\"], tags1, train_size=0.7, stratify=tags1, random_state=random.randint(0,100)\n",
    ")\n",
    "\n",
    "X_val_mc, X_test_mc, y_val_mc, y_test_mc = train_test_split(\n",
    "    X_mc, y_mc, train_size=0.5, stratify=y_mc, random_state=random.randint(0,100)\n",
    ")\n",
    "\"\"\"\n",
    "X_train_ml, X_ml, y_train_ml, y_ml = train_test_split(\n",
    "    X, data_labels, train_size=0.7, stratify=data_labels, random_state=random.randint(0,100)\n",
    ")\n",
    "\n",
    "X_val_ml, X_test_ml, y_val_ml, y_test_ml = train_test_split(\n",
    "    X_ml, y_ml, train_size=0.5, stratify=y_ml, random_state=random.randint(0,100)\n",
    ")\n",
    "\"\"\"\n",
    "#print(X_train_mc)\n",
    "#print(y_train_mc)\n",
    "#print(X_train_ml)\n",
    "#print(y_train_ml)\n",
    "\n",
    "print(f'train: {len(X_train_mc)} ({len(X_train_mc)/len(data):.0%})\\n'\n",
    "      f'val: {len(X_val_mc)} ({len(X_val_mc)/len(data):.0%})\\n'\n",
    "      f'test: {len(X_test_mc)} ({len(X_test_mc)/len(data):.0%})')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Importations and other preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import BaseEstimator\n",
    "#from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "#from skmultilearn.adapt import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we are using many models to compare the results, we can create a wrapper class so that we can use the pipeline to streamline the evocations needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClfSwitcher(BaseEstimator):\n",
    "\n",
    "    def __init__(self, estimator=RandomForestClassifier()):\n",
    "        \"\"\"\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator: sklearn object, the classifier\n",
    "        \"\"\"\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Pipeline (we might need to remove english stopwords as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8,\n",
    "                                   max_features=200000,\n",
    "                                   min_df=0.2,\n",
    "                                   stop_words=final_stopwords_list,\n",
    "                                   use_idf=True)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),\n",
    "    ('clf', ClfSwitcher())\n",
    "])"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Estimator grid and models to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grid = ParameterGrid({\n",
    "    'clf__estimator': [\n",
    "        MultiOutputClassifier(LogisticRegression(class_weight='balanced', random_state=random.randint(0,100), multi_class='multinomial', verbose=1), n_jobs=-1),\n",
    "        MultiOutputClassifier(SGDClassifier(class_weight='balanced',tol=1e-4, verbose=1,random_state=random.randint(0,100), loss='modified_huber', early_stopping=True, warm_start=True, average=True), n_jobs=-1),\n",
    "        MultiOutputClassifier(LinearSVC(dual=False, class_weight='balanced', random_state=random.randint(0,100), verbose=1), n_jobs=-1),\n",
    "        KNeighborsClassifier(p=2,n_jobs=-1),\n",
    "        RandomForestClassifier(class_weight='balanced', random_state=random.randint(0,100), n_jobs=-1, warm_start=True, verbose=1),\n",
    "        MultiOutputClassifier(XGBClassifier(scale_pos_weight=(1-y_train_mc.mean())/y_train_mc.mean(), random_state=random.randint(0,100), n_jobs=-1), n_jobs = -1),\n",
    "        MultiOutputClassifier(LGBMClassifier(is_unbalance=True, random_state=random.randint(0,100)), n_jobs=-1),\n",
    "        MultiOutputClassifier(Perceptron(class_weight='balanced',random_state=random.randint(0,100), n_jobs = -1, tol=1e-4, verbose=1, early_stopping=True))\n",
    "    ],\n",
    "    'tfidf__ngram_range': [(1,2), (1,4)]\n",
    "})\n",
    "\n",
    "models = [\n",
    "    'logreg1', 'logreg2', 'sgd1', 'sgd2', 'svm1', 'svm2', 'knn1', 'knn2', 'rf1', 'rf2',\n",
    "    'xgb1', 'xgb2', 'lgbm1', 'lgbm2', \"perc1\", \"perc2\"\n",
    "]\n",
    "\n",
    "#models1 = ['brknnA1', 'brknnA2', 'brknnB1', 'brknnB2']\n",
    "#grid1 = ParameterGrid({'clf__estimator':[BRkNNaClassifier(), BRkNNbClassifier()], 'k': range(1,3)})\n",
    "#models2 = ['mlknn1', 'mlknn2', 'mlnkk3', 'mlknn4', 'mlknn5', 'mlknn6']\n",
    "#grid2 = ParameterGrid({'clf__estimator': [MLkNN()], 'k': range(1,3), 's': [0.5, 0.7, 1.0]})\n",
    "#models3 = ['mltSVM1','mltSVM2','mltSVM3','mltSVM4','mltSVM5']\n",
    "#grid3 = ParameterGrid({'clf__estimator': [MLTSVM()], 'c_k': [2**i for i in range(-5, 5, 2)]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Predictions and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "def score(y_true, y_pred, index):\n",
    "    \"\"\"Calculate precision, recall, and f1 score\"\"\"\n",
    "\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    performance = {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n",
    "    return pd.DataFrame(performance, index=[index])\n",
    "\n",
    "for model, params in tqdm(zip(models, grid), total=len(models)):\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train_mc, y_train_mc)\n",
    "    y_pred = pipeline.predict(X_val_mc)\n",
    "    machine_learning = score(y_val_mc, y_pred, model)\n",
    "    scores = pd.concat([scores, machine_learning])\n",
    "\"\"\"\n",
    "for model, params in tqdm(zip(models1, grid1), total=len(models1)):\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train_ml, y_train_ml)\n",
    "    y_pred = pipeline.predict(X_val_ml)\n",
    "    machine_learning = score(y_val_ml, y_pred, model)\n",
    "    scores = pd.concat([scores, machine_learning])\n",
    "\n",
    "for model, params in tqdm(zip(models2, grid2), total=len(models2)):\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train_ml, y_train_ml)\n",
    "    y_pred = pipeline.predict(X_val_ml)\n",
    "    machine_learning = score(y_val_ml, y_pred, model)\n",
    "    scores = pd.concat([scores, machine_learning])\n",
    "\n",
    "for model, params in tqdm(zip(models3, grid3), total=len(models3)):\n",
    "    pipeline.set_params(**params)\n",
    "    pipeline.fit(X_train_ml, y_train_ml)\n",
    "    y_pred = pipeline.predict(X_val_ml)\n",
    "    machine_learning = score(y_val_ml, y_pred, model)\n",
    "    scores = pd.concat([scores, machine_learning])\n",
    "\"\"\"\n",
    "\n",
    "print(scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot(scores):\n",
    "    \"\"\"Plot scores of the models in a bar-plot based on f1 score\"\"\"\n",
    "\n",
    "    scores.sort_values('f1', ascending=False).plot(\n",
    "        kind='bar',\n",
    "        figsize=(12,5),\n",
    "        title='Score of the Models',\n",
    "        ylabel='score'\n",
    "    )\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.show()\n",
    "\n",
    "plot(scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results are, for the most part, off-putting at best. The main reason lies on the fact that the dataset is unbalanced, with most of the entries either pointing towards a rating label, a bug report one or a combination of both. We can take into consideration that the centralization of all label columns into a single one expressing the different combinations may be a source of error as well, because it transformed into a 15 label classification problem with only 6000 entries. But, because of time constraints, perceived complexity and most sources pointing to this approach as the standard, other ways were not explored."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data[\"tags\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otherwise, as the main language at work here is french, we do not have enough knowledge of which n-gram range is the most effective. One other thing to point out is that we were unable to incorporate the score as a feature of this dataset (only the text), which is a column we believe to be crucial in some cases to distinguish the review labels.\n",
    "For those reasons, we believe that a neural network approach is the most adequate to deal with this kind of problem, as that would solve some issues, but unfortunately cannot address all concerns."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
